# ğŸ§ª EvalRunnerAgent

A lightweight, Semantic Kernel-powered evaluation runner for testing LLM outputs against ground truth using similarity scoring.

---

## âœ… Current Features

- ğŸ¤– **LLM Integration** â€” Uses OpenAI GPT-4o for natural language responses.
- ğŸ“Š **Semantic Scoring** â€” Cosine similarity on text embeddings to assess match.
- ğŸ” **Retry Logic** â€” Auto-retries OpenAI calls up to 3 times.
- ğŸ’¾ **Eval Logging** â€” Saves results with timestamp to `Data/eval_results_<timestamp>.json`
- ğŸ§  **Score Thresholding** â€” Pass/fail determined via configurable threshold.
- ğŸ” **Configurable via `appsettings.json`**

---

## ğŸ”§ Example Output Summary

```bash
ğŸš€ Starting Evaluation Agent...
ğŸ§  Similarity Score: 0.9180
ğŸ§  Similarity Score: 0.9231
ğŸ§  Similarity Score: 0.8108

Results saved to Data/eval_results_20250429T151745.json
âœ… Passed: 2 | âŒ Failed: 1
```

